{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDPMmAYp9yOY"
      },
      "outputs": [],
      "source": [
        "#1.Create a Doc object from the file owlcreek.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the text file\n",
        "with open(\"/content/owlcreek.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Create the Doc object\n",
        "doc = nlp(text)\n",
        "\n",
        "# Example usage: Print sentences or tokens\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "\n",
        "# Create the Doc object\n",
        "doc = nlp(text)\n",
        "\n",
        "# Example usage: Print sentences or tokens\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "id": "lDuvnT2E-I7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.How many tokens are contained in the file?"
      ],
      "metadata": {
        "id": "xgurvs1V_Ip1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "num_tokens = len(doc)\n",
        "print(\"Number of tokens:\", num_tokens)"
      ],
      "metadata": {
        "id": "52quemmw_Oh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.How many sentences are contained in the file?"
      ],
      "metadata": {
        "id": "PHIl1i2J_h5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sentences = len(list(doc.sents))\n",
        "print(\"Number of sentences:\", num_sentences)"
      ],
      "metadata": {
        "id": "QlxCnNr-_r55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Print the second sentence in the document"
      ],
      "metadata": {
        "id": "E11jxCKlAG-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = list(doc.sents)\n",
        "second_sentence = sentences[1].text\n",
        "\n",
        "print(\"Second sentence:\")\n",
        "print(second_sentence)"
      ],
      "metadata": {
        "id": "8GEHyYpDALoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. For each token in the sentence above, print its text, POS tag, dep tag and lemma."
      ],
      "metadata": {
        "id": "QV0bsc6GAjSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "sentences = list(doc.sents)\n",
        "second_sentence = sentences[1]\n",
        "\n",
        "\n",
        "print(f\"{'Text':<15}{'POS':<10}{'Dep':<15}{'Lemma':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "for token in second_sentence:\n",
        "    print(f\"{token.text:<15}{token.pos_:<10}{token.dep_:<15}{token.lemma_:<15}\")"
      ],
      "metadata": {
        "id": "hWwuIqq8HJLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text."
      ],
      "metadata": {
        "id": "gGKZ8el1HUzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LOWER\": \"swimming\"}, {\"LOWER\": \"vigorously\"}]\n",
        "matcher.add(\"Swimming\", [pattern])\n",
        "text = \"I enjoy swimming vigorously. Swimming vigorously is great for fitness. Many people love swimming vigorously.\"\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "found_matches = [doc[start:end].text for match_id, start, end in matches]\n",
        "print(\"Found matches:\", found_matches)"
      ],
      "metadata": {
        "id": "sJmnG9xMHXV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}